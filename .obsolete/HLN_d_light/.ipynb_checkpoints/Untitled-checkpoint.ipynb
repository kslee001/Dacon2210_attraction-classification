{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37014b41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX2\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X2' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2411db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job started -- current model : HLNd\n",
      "data loaded...\n",
      "execute data augmentation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87434e558694ddd8d70842ddc0470b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:/내 드라이브/codes/python/dacon/data/original/image/train/TRAIN_04958.jpg\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m start_time \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m X1, X2, y1, y2, y3   \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_augmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdo_augmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdo_embedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mgenerate_dataloader(X1, X2, y1, y2, y3)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# define model\u001b[39;00m\n",
      "File \u001b[1;32mG:\\내 드라이브\\codes\\python\\dacon\\Dacon\\HLN_d\\functions\\preprocessing.py:270\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(do_augmentation, do_embedding)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# (execute augmentation)\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_augmentation:\n\u001b[1;32m--> 270\u001b[0m     num_aug_data \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     num_aug_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maug_img_train\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32mG:\\내 드라이브\\codes\\python\\dacon\\Dacon\\HLN_d\\functions\\preprocessing.py:186\u001b[0m, in \u001b[0;36mload_data.<locals>.execute_augmentation\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(path)\n\u001b[0;32m    185\u001b[0m cur_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(path)\n\u001b[1;32m--> 186\u001b[0m cur_img \u001b[38;5;241m=\u001b[39m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m cur_img \u001b[38;5;241m=\u001b[39m normalize(augment(cur_img))\n\u001b[0;32m    188\u001b[0m cur_img \u001b[38;5;241m=\u001b[39m to_tensor(cur_img)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mG:\\내 드라이브\\codes\\python\\dacon\\Dacon\\HLN_d\\functions\\preprocessing.py:87\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize\u001b[39m(x):\n\u001b[1;32m---> 87\u001b[0m     H, W, C \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m H\u001b[38;5;241m>\u001b[39mW:\n\u001b[0;32m     89\u001b[0m         tf \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     90\u001b[0m             A\u001b[38;5;241m.\u001b[39mResize(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMG_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39mH\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mW, CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMG_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m     91\u001b[0m         ])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# config & dataset\n",
    "# import os\n",
    "# os.chdir(\"/home/gyuseonglee/.Dacon/workplace/TNBranchC1\")\n",
    "\n",
    "from config import *\n",
    "from model.hierarchical_loss import HierarchicalLossNetwork\n",
    "import functions.preprocessing as preprocessing\n",
    "import functions.train as train\n",
    "from model.models import *\n",
    "\n",
    "import torch\n",
    "import time as t\n",
    "import subprocess\n",
    "import json\n",
    "import pprint\n",
    "DEFAULT_ATTRIBUTES = (\n",
    "    'index',\n",
    "    'uuid',\n",
    "    'name',\n",
    "    'timestamp',\n",
    "    'memory.total',\n",
    "    'memory.free',\n",
    "    'memory.used',\n",
    "    'utilization.gpu',\n",
    "    'utilization.memory'\n",
    ")\n",
    "def get_gpu_info(nvidia_smi_path='nvidia-smi', keys=DEFAULT_ATTRIBUTES, no_units=True):\n",
    "    nu_opt = '' if not no_units else ',nounits'\n",
    "    cmd = '%s --query-gpu=%s --format=csv,noheader%s' % (nvidia_smi_path, ','.join(keys), nu_opt)\n",
    "    output = subprocess.check_output(cmd, shell=True)\n",
    "    lines = output.decode().split('\\n')\n",
    "    lines = [ line.strip() for line in lines if line.strip() != '' ]\n",
    "\n",
    "    return [ { k: v for k, v in zip(keys, line.split(', ')) } for line in lines ]\n",
    "\n",
    "def gpu_mem():\n",
    "    pprint.pprint(get_gpu_info())\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # try:\n",
    "    print(f\"job started -- current model : {folder_name}\")\n",
    "    start_time = t.time()\n",
    "\n",
    "    # load data\n",
    "    X1, X2, y1, y2, y3   = preprocessing.load_data(\n",
    "        do_augmentation=DATA[\"do_augmentation\"], \n",
    "        do_embedding=DATA[\"do_embedding\"]\n",
    "    )\n",
    "    train_loader, val_loader = preprocessing.generate_dataloader(X1, X2, y1, y2, y3)\n",
    "    \n",
    "\n",
    "    # define model\n",
    "    model     = HLNd().cuda()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params       = model.parameters(), \n",
    "        lr           = CFG[\"LEARNING_RATE\"],\n",
    "    ) \n",
    "    criterion = HierarchicalLossNetwork()\n",
    "\n",
    "\n",
    "    # parallelize model\n",
    "    # model = torch.nn.DataParallel(model, device_ids = [0,1])\n",
    "    model.eval()\n",
    "\n",
    "    warm_up = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, \n",
    "        start_factor=CFG[\"LEARNING_RATE\"]/5, \n",
    "        end_factor=CFG[\"LEARNING_RATE\"], \n",
    "        total_iters=5\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        \"max\", \n",
    "        **scheduler_args\n",
    "    )\n",
    "\n",
    "    print(\"train started...\")\n",
    "    train.make_dir(output_folder)\n",
    "    best_model = train.train(\n",
    "        model=model, \n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        warm_up=warm_up,\n",
    "        scheduler=scheduler,\n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader\n",
    "    )\n",
    "\n",
    "    # save model\n",
    "    train.save_model(best_model)\n",
    "    # save configs\n",
    "    train.save_configs()\n",
    "    \n",
    "    \n",
    "\n",
    "    end_time = t.time()\n",
    "    duration = (end_time-start_time)\n",
    "    \n",
    "    h = int(duration//3600)\n",
    "    m = int(duration//60 - h*60)\n",
    "    s = round(duration%60,3)\n",
    "\n",
    "    print(\"job finished\")\n",
    "    print(f\"duration : {h} h {m} m {s} s\")\n",
    "    gpu_mem()\n",
    "    \n",
    "    # except:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
